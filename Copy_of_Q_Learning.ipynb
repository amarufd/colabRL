{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarufd/colabRL/blob/master/Copy_of_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yxbELmQZOmi",
        "colab_type": "text"
      },
      "source": [
        "# Diplomado de Inteligencia Artificial\n",
        "Por Juan-Pablo Silva (jpsilva@dcc.uchile.cl) y Alexandre Bergel (abergel@dcc.uchile.cl)\n",
        "\n",
        "Integrantes del grupo:\n",
        "\n",
        "\n",
        "*   Integrante 1\n",
        "*   Integrante 2\n",
        "*   Integrante 3\n",
        "\n",
        "\n",
        "\n",
        "## Reinforcement Learning\n",
        "\n",
        "El reinforcement learning es una tecnica en el aprendizaje de maquinas donde no damos feedback inmediatas a las maquinas, en este contexto comunmente referidas **agentes**, sino que dejamos se exploren y se desenvuelvan en el mundo. Luego, mientras se desenvuelven, damos recompensas si hacen una buena accion, y una penalizacion si hacen algo malo. Esto suena muy similar al aprendizaje supervisado que ya han visto con redes neuronales, pero hay una diferencia bastante sustancial.\n",
        "\n",
        "Las redes neuronales en aprendizaje supervisado reciben feedback, o una evaluacion de lo que han hecho, por **cada** accion que realizan. En un video juego por ejemplo, es dificil saber si dar un paso hacia adelante es algo positivo o negativo, por lo tanto es muy dificil saber si la accion es correcta o no; ese paso hacia adelante pudo haber influido en hacernos ganar, o perder el juego, imposible saber en ese momento.\n",
        "En cambio, con reinforcement learning damos feedback por acciones **buenas** y castigamos acciones **malas**, no nos preocupamos por acciones *intermedias* que no estamos seguros a donde nos pueden llevar, eso es parte de lo que el agente debe aprender. Es por esto que se llama *aprendizaje reforzado*, u otro nombre quizas mas adecuado, aprendizaje con feedback retardado, porque le decimos despues de que hizo las acciones si estas estan bien o no.\n",
        "\n",
        "## Actividades\n",
        "\n",
        "En este *notebook* hay varias actividades que realizaremos en los 2 dias que dura el topico de *Reinforcement Learning*. Aqui encontras preguntas que debes responder, y ejercicios que contaran como nota del topico. Estas secciones con nota seran etiquetadas con **Pregunta**.\n",
        "\n",
        "Primero introduciremos el ambiente en el que nos desenvolveremos. Existen librerias como *Gym* que presentan ambientes de videojuegos para probar y evaluar tecnicas de *reinforcemente learning*, pero son una caja negra para la mayoria de los casos y entrar al codigo es bastante complejo. Para mitigar esto, hemos diseñado nuestro propio ambiente, muy simple, que nos permitira jugar un pequeño juego donde tenemos un **heroe** 🙃 que necesita recoger **trofeos** 🏆 y evitar a los **zombies** 🧟."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPmfPbg8bVSE",
        "colab_type": "text"
      },
      "source": [
        "## Ambiente para nuestro agente\n",
        "\n",
        "Aqui implementaremos las partes necesarias para que nuestro agente pueda vivir en este mundo.\n",
        "\n",
        "\n",
        "Primero necesitamos importar algunas cosas para facilitarnos la vida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJaoDoPXMzCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy para copiar nuestros objetos de un lado para otro\n",
        "from copy import deepcopy, copy\n",
        "# numpy es una libreria numerica que permite facil trabajo con \n",
        "# matrices, como matlab. La usamos para trabajos numericos\n",
        "import numpy as np\n",
        "# para tener cosas random!\n",
        "import random\n",
        "# para ayudarnos con los tipos, nos ayuda a tener mas claro que retorna que\n",
        "from typing import List, Tuple, Any, Union, NewType, Dict\n",
        "\n",
        "# siempre es bueno usar una semilla cuando hacemos experimentos, es la unica\n",
        "# forma confiable que tenemos para asegurarnos que nuestros experimentos \n",
        "# son reproducibles\n",
        "random.seed(42)  # no importa que numero elijamos, pero lo dejamos fijo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFBSvEohM8FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vamos a definir algunas cosas con `monitos` para que nuestro mapa se\n",
        "# vea mas entretenido\n",
        "\n",
        "# Partiremos con estos\n",
        "ZOMBIE = \"🧟\"\n",
        "HERO = \"🙃\"\n",
        "TROPHIE = \"🏆\"\n",
        "EMPTY = \"⚪\"\n",
        "\n",
        "# despues ustedes tendran que agregar estos!\n",
        "BLOCK = \"🧱\"\n",
        "KEY = \"🔑\"\n",
        "DOOR = \"🚪\"\n",
        "SWORD = \"🗡️\"\n",
        "\n",
        "\n",
        "# Nuestro agente tiene que saber las condiciones del mundo donde existe\n",
        "# en este mundo solo hay 4 acciones que puede hacer.\n",
        "# Moverse hacia `arriba`, `abajo`, `derecha` e `izquierda`, nada mas.\n",
        "# En algun otro ambiente, podriamos agregar mas acciones como saltar\n",
        "# atacar, comprar, etc. Pero mantendremos la simplicidad aqui.\n",
        "UP = 0\n",
        "DOWN = 1\n",
        "LEFT = 2\n",
        "RIGHT = 3\n",
        "# Juntamos nuestras acciones para que queden ordenadas.\n",
        "ACTIONS = [UP, DOWN, LEFT, RIGHT]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuyp-xM_leVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Aqui vamos a crear nuestros tipos, esto nos ayudara a entender que hace\n",
        "# cada metodo y funcion que usemos mas claramente.\n",
        "Action = NewType('Action', int)\n",
        "GridElement = NewType('GridElement', str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GApT6pfUdZZD",
        "colab_type": "text"
      },
      "source": [
        "Arriba hemos definimos nuestro mapa. Esto no es importante para entender el concepto de *reinforcement learning*, sino mas bien es una pura implementacion a mano de un mapa. Puedes saltarte toda la parte donde creamos la grilla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p62m4TTf1vz",
        "colab_type": "text"
      },
      "source": [
        "# Grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j36sWY4KMzQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Aqui definimos nuestra grilla que acturara como la base de nuestro mapa\n",
        "class Grid:\n",
        "    # Nuestro constructor toma una lista u otra grilla y la guarda\n",
        "    # se preocupa de copiarla para que no modifiquemos la anterior\n",
        "    def __init__(self, grid:Union['Grid', List[List[GridElement]]]=None) -> None:\n",
        "        assert grid is not None\n",
        "        if isinstance(grid, list):\n",
        "            self.grid = deepcopy(grid)\n",
        "        elif isinstance(grid, Grid):\n",
        "            self.grid = deepcopy(grid.grid)\n",
        "            \n",
        "        # Guardamos el tamaño de la grilla para trabajar mas rapido\n",
        "        self.x_lim = len(self.grid[0])\n",
        "        self.y_lim = len(self.grid)\n",
        "        \n",
        "    # Nuestro metodo para comparar una grilla con otra\n",
        "    def __eq__(self, other:'Grid') -> bool:\n",
        "        return isinstance(other, Grid) and self.grid == other.grid\n",
        "    \n",
        "    # Simpre es importante que si modificamos nuestra igualdad, tambien\n",
        "    # adaptemos nuestro hash\n",
        "    def __hash__(self) -> int:\n",
        "        return hash(str(self.grid))\n",
        "    \n",
        "    # Cuando imprimimos una grilla, esta se mostrara como un\n",
        "    # mapa, como una matriz\n",
        "    def __str__(self) -> str:\n",
        "        return '\\n'.join([' '.join(str(e) for e in row) for row in self.grid])\n",
        "    \n",
        "    # Este es un metodo muy util para indexar partes de la grilla\n",
        "    def __getitem__(self, position:Tuple[int, int]) -> GridElement:\n",
        "        assert type(position) == tuple\n",
        "        # necesitamos 2 coordenadas para saber que hay en esa posicion\n",
        "        assert len(position) == 2\n",
        "        x, y = position\n",
        "        # verificamos que las coordenadas esten dentro de la grilla\n",
        "        assert 0 < x <= self.x_lim\n",
        "        assert 0 < y <= self.y_lim\n",
        "        # retornamos el elemento que hay en esa posicion\n",
        "        return self.grid[self.y_lim-y][x-1]\n",
        "    \n",
        "    # Este es un metodo muy util para insertar elementos en la grilla\n",
        "    def __setitem__(self, position:Tuple[int, int], value:GridElement) -> None:\n",
        "        assert type(position) == tuple\n",
        "        assert len(position) == 2\n",
        "        x, y = position\n",
        "        assert 0 < x <= self.x_lim\n",
        "        assert 0 < y <= self.y_lim\n",
        "        # igual que antes, pero ahora asignamos un elemento en vez de retornarlo\n",
        "        self.grid[self.y_lim-y][x-1] = value\n",
        "        \n",
        "    # Una forma `fancy` de acceder a variables de una clase sin un `getter`\n",
        "    @property\n",
        "    def shape(self) -> Tuple[int, int]:\n",
        "        return (self.x_lim, self.y_lim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rx9v-DPixO-",
        "colab_type": "text"
      },
      "source": [
        "Lo anterior pueden ignorarlo completamente, pero ahora si parte lo que nos importa!\n",
        "\n",
        "A continuacion definiremos una clase `State`, la cual representa el estado de nuestro agente en algun momento de su travesia. \n",
        "Un estado tiene 3 cosas:\n",
        "\n",
        "\n",
        "1.   El mapa en ese momento. Es decir, tenemos una grilla dentro del estado. Esto representara el estado del mapa en cada instante.\n",
        "2.   Las posiciones de nuestro **heroe**, claramente necesitamos saber donde esta nuestro personaje en cada momento.\n",
        "3.   Los limites del mapa, para no salirnos del mapa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHFdUCE3gcWP",
        "colab_type": "text"
      },
      "source": [
        "# State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L-t7bAufqw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# La clase `State` representara un estado del heroe\n",
        "class State:\n",
        "    # En nuestro constructor solo asignamos nuestras variables\n",
        "    def __init__(self, grid:Union[Grid, List[List[GridElement]]]=None, \n",
        "                 hero_pos:Tuple[int,int]=(1,1)) -> None:\n",
        "        \n",
        "        self.grid = Grid(grid=grid)\n",
        "        self.x_lim, self.y_lim = self.grid.shape\n",
        "        self.hero_x, self.hero_y = hero_pos\n",
        "        \n",
        "    # Misma forma `fancy` de acceder a la posicion del heroe\n",
        "    @property\n",
        "    def hero_pos(self) -> Tuple[int, int]:\n",
        "        return (self.hero_x, self.hero_y)\n",
        "    \n",
        "    # Para dibujar nuestro mapa con el heroe en la posicion actual\n",
        "    def __str__(self) -> str:\n",
        "        grid = deepcopy(self.grid)\n",
        "        grid[self.hero_x, self.hero_y] = HERO\n",
        "        return grid.__str__()\n",
        "    \n",
        "    # Un estado es igual a otro si las grillas y posiciones de los heroes son\n",
        "    # las mismas\n",
        "    def __eq__(self, other:'State') -> bool:\n",
        "        return isinstance(other, State) and self.hero_pos == other.hero_pos and \\\n",
        "            self.grid == other.grid\n",
        "    \n",
        "    # Igual que antes, por completitud debemos implementar cuando 2\n",
        "    # estados tienen el mismo hash\n",
        "    def __hash__(self) -> int:\n",
        "        return hash(str(self.grid) + str(self.hero_pos))\n",
        "    \n",
        "    # Este metodo nos ayuda a obtener que elemento se encuentra en una posicion\n",
        "    # determinada. Necesitamos el estado pasado para comparar ya que no tenemos\n",
        "    # historia, pero es una forma simple de implementar el mapa sin\n",
        "    # mucho codigo. Recuerda que estamos en una cadena de Markov, aqui no tenemos\n",
        "    # los estados pasados! Estos no afectan la decision que tomaremos ahora.\n",
        "    def get_element(self, position:Tuple[int,int], state:'State') -> GridElement:\n",
        "        assert type(position) == tuple\n",
        "        assert len(position) == 2\n",
        "        x, y = position\n",
        "        assert 0 < x <= self.x_lim\n",
        "        assert 0 < y <= self.y_lim\n",
        "        \n",
        "        # Por limitaciones de la implementacion, debemos saber si el heroe se\n",
        "        # movio a la posicion en la que esta, o estaba ahi desde antes\n",
        "        # otra implementacion podria solucionar este problema de mejor manera\n",
        "        # pero es mas compleja de entender\n",
        "        if position == state.hero_pos:\n",
        "            return HERO\n",
        "        return self.grid[x,y]\n",
        "        \n",
        "    # De nuestras acciones tenemos que elegir una y actuar acorde a ella.\n",
        "    # por ejemplo, si le pedimos al estado que suba, entonces tenemos que \n",
        "    # enviar la accion `UP`.\n",
        "    # Cuando llamamos a este metodo, creamos un nuevo estado con la\n",
        "    # accion aplicada\n",
        "    def action_dispatch(self, action:Action) -> 'State':\n",
        "        if action == UP:\n",
        "            return self.moveUp()\n",
        "        elif action == DOWN:\n",
        "            return self.moveDown()\n",
        "        elif action == LEFT:\n",
        "            return self.moveLeft()\n",
        "        elif action == RIGHT:\n",
        "            return self.moveRight()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown action {action}\")\n",
        "        \n",
        "    # Este metodo solo copia el estado actual y crea uno nuevo para aplicar\n",
        "    # los cambios pedidos por la accion ingresada\n",
        "    def register(self) -> 'State':\n",
        "        past_state = copy(self)\n",
        "        return State(grid=past_state.grid, hero_pos=past_state.hero_pos)\n",
        "    \n",
        "    # Los siguientes metodos mueven nuestro personaje en las direcciones\n",
        "    # que definimos antes, arriba, abajo, derecha e izquierda\n",
        "        \n",
        "    def moveUp(self) -> 'State':\n",
        "        new_state = self.register()\n",
        "        #posicio =(new_state.hero_x, new_state.hero_y + 1 if new_state.hero_y < new_state.y_lim else new_state.hero_y)\n",
        "        #print(posicio)\n",
        "        #print(self.get_element(posicio,self))\n",
        "        new_state.hero_y = new_state.hero_y + 1 if new_state.hero_y < new_state.y_lim else new_state.hero_y\n",
        "        \n",
        "        return new_state\n",
        "        \n",
        "    def moveDown(self) -> 'State':\n",
        "        new_state = self.register()\n",
        "        #posicio =(new_state.hero_x, new_state.hero_y - 1 if new_state.hero_y > 1 else new_state.hero_y)\n",
        "        #print(posicio)\n",
        "        #print(self.get_element(posicio,self))\n",
        "        new_state.hero_y = new_state.hero_y - 1 if new_state.hero_y > 1 else new_state.hero_y\n",
        "        \n",
        "        return new_state\n",
        "        \n",
        "    def moveRight(self) -> 'State':\n",
        "        new_state = self.register()\n",
        "        #posicio =(new_state.hero_x + 1 if new_state.hero_x < new_state.x_lim else new_state.hero_x, new_state.hero_y)\n",
        "        #print(posicio)\n",
        "        #print(self.get_element(posicio,self))\n",
        "        new_state.hero_x = new_state.hero_x + 1 if new_state.hero_x < new_state.x_lim else new_state.hero_x\n",
        "        \n",
        "        return new_state\n",
        "        \n",
        "    def moveLeft(self) -> 'State':\n",
        "        new_state = self.register()\n",
        "        #posicio =(new_state.hero_x - 1 if new_state.hero_x > 1 else new_state.hero_x, new_state.hero_y)\n",
        "        #print(posicio)\n",
        "        #print(self.get_element(posicio,self))\n",
        "        new_state.hero_x = new_state.hero_x - 1 if new_state.hero_x > 1 else new_state.hero_x\n",
        "        \n",
        "        return new_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g70vnhBcm1Bd",
        "colab_type": "text"
      },
      "source": [
        "Listo, con esto hemos definido las bases para que nuestro **agente** pueda moverse libremente por el mundo que creemos. \n",
        "Probemos a ver como funciona!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbMhqHTpqDEs",
        "colab_type": "code",
        "outputId": "6e4428a0-e923-414a-dc6a-998940c0c6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Creamos una lista de listas (una matriz) que represente a nuestro mapa\n",
        "mapa_ejemplo = [\n",
        "    [TROPHIE, EMPTY, EMPTY],\n",
        "    [ZOMBIE, EMPTY, ZOMBIE],\n",
        "    [EMPTY, EMPTY, EMPTY]\n",
        "]\n",
        "# Digamos que nuestro heroe parte en la posicion (1,1)\n",
        "estado_ejemplo = State(grid=mapa_ejemplo, hero_pos=(1, 1))\n",
        "\n",
        "# Veamos como se ve!\n",
        "print(estado_ejemplo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 ⚪ ⚪\n",
            "🧟 ⚪ 🧟\n",
            "🙃 ⚪ ⚪\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw4LeuzVqCBE",
        "colab_type": "text"
      },
      "source": [
        "Ahi esta nuestro **heroe** 🙃! Tambien podemos ver nuestro trofeo 🏆 a cual queremos llegar, y los zombies 🧟 que debemos evitar en el camino. Vamos a representar el camino libre como un circulo blanco ⚪.\n",
        "\n",
        "Ahora empieza la parte de aprendizaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9xgY1HWnBRJ",
        "colab_type": "text"
      },
      "source": [
        "## Feedback para el **agente**\n",
        "\n",
        "Hemos creado nuestro estado, nuestro mapa y todo, pero ahora necesitamos de alguna forma ver que acciones merecen un premio para el **agente** y cuales un castigo.\n",
        "\n",
        "- [x] Mapa\n",
        "- [x] Definicion de un estado\n",
        "- [ ] Cuando dar recompensas y cuando castigar\n",
        "- [ ] Aprender...\n",
        "\n",
        "Esto lo definiremos en una funcion que llamaremos `act`. La funcion `act` necesita un `State` `s` y un `Action` `a` como argumentos, para simular un movimiento, desde un estado `s` mediante la accion `a`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPi4H0QPMzTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos una funcion que represente un `acto`. Es decir\n",
        "# dado un estado y una accion, que ocurre.\n",
        "# Este \"que ocurre\" es bastante variado, podemos movernos,\n",
        "# ganar puntaje, perder el juego, o cualquier cosa que decidamos\n",
        "\n",
        "# Aqui es donde debemos decidir cuando y cuanta recompensa o castigo\n",
        "# debemos dar a nuestro agente.\n",
        "# Esta funcion retornara 3 cosas. El nuevo estado en que quedo nuestro heroe,\n",
        "# una recompensa por su esfuerzo (puede ser negativa), y un booleano indicando\n",
        "# si el juego termino o no. Este `termino` puede ser porque ganamos o perdimos.\n",
        "def act(state:State, action:Action, condition=None): \n",
        "    \n",
        "    # Le decimos a nuestro estado que se mueva en la direccion pedida\n",
        "    # esto nos da un nuevo estado\n",
        "    new_state = state.action_dispatch(action)\n",
        "    \n",
        "    # ahora le pedimos al nuevo estado que nos diga que hay \n",
        "    # en la posicion que quedamos\n",
        "    # De nuevo, por un tema de implementacion tenemos que saber si donde estamos\n",
        "    # ahora estaba ocupado por otro elemento, o si siempre estuvimos nosotros ahi.\n",
        "    grid_item = new_state.get_element(new_state.hero_pos, state)\n",
        "    \n",
        "    # Si nos encontramos un zombie, que hacemos?\n",
        "    if grid_item == ZOMBIE:\n",
        "        \n",
        "        # como no tenemos como defendernos, perdemos\n",
        "        # le daremos una recompensa negativa al agente por que se equivoco\n",
        "        \n",
        "        # le daremos -100 como recompensa.\n",
        "        #reward = -100\n",
        "        # el juego se acabo...\n",
        "        #is_done = True\n",
        "        reward,is_done= condition(ZOMBIE)\n",
        "        new_state.grid[new_state.hero_pos] = EMPTY \n",
        "        \n",
        "    # Si nos encontramos con un trofeo, que hacemos?\n",
        "    elif grid_item == TROPHIE:\n",
        "        # pues ganamos! Le damos 1000 de recompensa al agente\n",
        "        \n",
        "        # le daremos 1000 como recompensa.\n",
        "        #reward = 1000\n",
        "        # el juego se acabo... pero ahora ganamos!\n",
        "        #is_done = True\n",
        "        reward,is_done= condition(TROPHIE)\n",
        "        new_state.grid[new_state.hero_pos] = EMPTY \n",
        "        \n",
        "    # Si el espacio esta vacio, que hacemos?\n",
        "    elif grid_item == EMPTY:\n",
        "        # nada, simplemente nos movemos a ese lugar.\n",
        "        \n",
        "        # por que es negativa la recompensa? Lo veremos en la pregunta 7!\n",
        "        #reward = -1\n",
        "        # no se ha terminado el juego\n",
        "        #is_done = False\n",
        "        reward,is_done= condition(EMPTY)\n",
        "    \n",
        "    # Si el heroe ya estba en ese espacio, que hacemos?\n",
        "    elif grid_item == HERO:\n",
        "        # nada, simplemente nos quedamos igual.\n",
        "        \n",
        "        # por que es negativa la recompensa? Lo veremos en la pregunta 7!\n",
        "        #reward = -1\n",
        "        # no se ha terminado el juego\n",
        "        #is_done = False\n",
        "        reward,is_done= condition(HERO)\n",
        "        \n",
        "    elif grid_item == BLOCK:\n",
        "        # nada, simplemente nos quedamos igual.\n",
        "        \n",
        "        # por que es negativa la recompensa? Lo veremos en la pregunta 7!\n",
        "        #reward = -1\n",
        "        # no se ha terminado el juego\n",
        "        #is_done = False\n",
        "        reward,is_done= condition(BLOCK)\n",
        "        new_state = state\n",
        "    \n",
        "    elif grid_item == DOOR:\n",
        "      \n",
        "        reward,is_done = condition(DOOR)\n",
        "        if (reward > 0):\n",
        "          new_state.grid[new_state.hero_pos] = EMPTY\n",
        "        else:\n",
        "          new_state = state\n",
        "    \n",
        "    elif grid_item == KEY:\n",
        "      \n",
        "        reward,is_done= condition(KEY)\n",
        "        new_state.grid[new_state.hero_pos] = EMPTY \n",
        "        \n",
        "    elif grid_item == SWORD:\n",
        "      \n",
        "        reward,is_done= condition(SWORD)\n",
        "        new_state.grid[new_state.hero_pos] = EMPTY \n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown grid item {grid_item}\")\n",
        "    \n",
        "    return new_state, reward, is_done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVZoxljBvHrE",
        "colab_type": "text"
      },
      "source": [
        "## Listos para aprender!\n",
        "\n",
        "Ahora tenemos todo nuestro ambiente implementado y definido.\n",
        "\n",
        "- [x] Mapa\n",
        "- [x] Definicion de un estado\n",
        "- [x] Cuando dar recompensas y cuando castigar\n",
        "- [ ] Aprender...\n",
        "\n",
        "Lo que necesitamos ahora es algun algoritmo que nos ayude a aprender que acciones son buenas y cuales no. Para esto usaremos un algoritmo llamado *Q-Learning*, que es lo que vimos en la clase teorica antes.\n",
        "\n",
        "Para esto, primero necesitamos una tabla donde iremos guardando cada uno de los estados y sus puntajes para cada accion. Es decir, dado un estado `s`, que deberiamos hacer ahora. Esta decision se toma de acuerdo a un puntaje que va asociado a cada accion dado cada estado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPzvQBy1wuQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declaramos nuestra tabla `q_table` como un diccionario vacio\n",
        "# Nuestra tabla se vera de la siguiente forma:\n",
        "# estado: [lista de acciones posibles]\n",
        "# Esta lista de acciones posibles es una lista de puntajes para cada\n",
        "# accion dado un estado.\n",
        "q_table = {}\n",
        "\n",
        "# Luego hacemos nuestra funcion de busqueda `q`.\n",
        "# Esta tiene 2 funciones:\n",
        "# 1. Dado  un estado, retorna una lista con los puntajes para cada accion en ese\n",
        "# estado. Es decir, una lista con puntajes para decidir que hacer\n",
        "# 2. Dado un estado y una accion, retorna el puntaje asociado a realizar\n",
        "# esa accion en ese estado.\n",
        "def q(state:State, action:Action=None) -> Union[float, np.ndarray]:\n",
        "    if state not in q_table:\n",
        "        # Si no hemos visto este estado, lo creamos\n",
        "        # como no sabemos que hacer aun, decimos que todas las acciones\n",
        "        # tienen beneficio 0, ya que no lo hemos evaluado aun\n",
        "        q_table[state] = np.zeros(len(ACTIONS))\n",
        "        \n",
        "    if action is None:\n",
        "        return q_table[state]\n",
        "    \n",
        "    return q_table[state][action]\n",
        "\n",
        "# Este es un metodo conveniente para no estar borrando manualmente\n",
        "# la tabla cada vez que queremos hacer algo nuevo\n",
        "def reset_table():\n",
        "    q_table = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q9_KqVHNFFO",
        "colab_type": "text"
      },
      "source": [
        "Para terminar nuestras configuraciones, es necesario que digamos cuantas veces intentaremos correr el juego, y tambien la duracion del juego. Por ejemplo, podriamos estar jugando 1 hora por 2 semanas, o 14 horas en un puro dia. Para asimilar lo aprendido, primero debemos dormir, es decir, dejar de jugar. \n",
        "\n",
        "Este ejemplo de dormir y tiempo entre juegos es una analogia muy util para describir el concepto de `episodios` y `pasos` de nuestro **agente**. Por cada `episodio` nuestro agente comienza el juego denuevo y podemos darnos cuenta si mejoro o no, entonces no nos sirve solo hacer un `episodio` super largo si no tendremos la oportunidad de verificar los resultados; pero si tenemos demasiados episodios, no terminaremos de jugar nunca. El \"largo\" del juego viene dado por los `pasos`. Si son muy pocos `pasos` puede que no alcancemos a aprender lo que queremos, pero si son muchos puede que estemos perdiendo el tiempo y ya hayamos encontrado lo que buscabamos.\n",
        "\n",
        "A continuacion definiremos estas constantes para nuestro problema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot-A9d6kMzYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# El total de episodios donde nuestro agente aprendera\n",
        "N_EPISODES = 50\n",
        "\n",
        "# El maximo numero de pasos por episodio\n",
        "MAX_EPISODE_STEPS = 10000000000\n",
        "\n",
        "# Debemos definir nuestro conjunto de pesos de entrenamiento.\n",
        "# En un comienzo nuestro agente aprendera mucho, ya que sus primeros\n",
        "# acercamientos al juego son mas valiosos. Pero mientras mas veces jugamos\n",
        "# lo que aprendemos por cada jugada es cada vez menos. Es importante hacer esta\n",
        "# diferencia o una jugada muy avanzada, por intentar explorar, podria\n",
        "# arruinar todo lo que habiamos aprendido antes.\n",
        "\n",
        "# Siempre aprenderemos aun que sea un poco\n",
        "MIN_ALPHA = 0.02\n",
        "# Aprenderemos desde TODO, hasta un 2% de lo que veamos.\n",
        "# Esta decision es arbitraria, intenten cambiarlo a ver que pasa!\n",
        "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES)\n",
        "\n",
        "# Un factor de descuento. Esto lo usamos para balancear entre la recompensa\n",
        "# maxima a corto plazo, o a largo plazo. Si lo dejamos solo a corto plazo\n",
        "# es poco probable que aprendamos algo util a futuro. Pero si lo dejamos en\n",
        "# 100% entonces estamos pensando demasiado en el futuro y no nos estamos\n",
        "# preocupando del presente.\n",
        "# Generalmente este valor esta entre 80 y 99%\n",
        "gamma = 0.9\n",
        "\n",
        "# Si solo nos guiamos por la mejor accion y no exploramos ni nos arriesgamos\n",
        "# es poco probable que aprendamos mucho del mundo. Por esto, es importante\n",
        "# poner un poco de aleatoriedad en esto. Existe un 20% de probabilidades\n",
        "# de que elijamos una accion al azar dado el estado que estamos. En contraste\n",
        "# existe un 80% de probabilidad de que elijamos la mejor accion que conocemos.\n",
        "eps = 0.2\n",
        "\n",
        "\n",
        "# Aqui simulamos la eleccion de una accion. Dado un estado, nos dice que\n",
        "# accion tomar. Existe un `eps` probabilidad de que tomemos una accion\n",
        "# al azar.\n",
        "def choose_action(state:State) -> Action:\n",
        "    if random.random() < eps:\n",
        "        return random.choice(ACTIONS) \n",
        "    else:\n",
        "        return np.argmax(q(state))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o83QfpM9S6Cr",
        "colab_type": "text"
      },
      "source": [
        "Listo! Implementemos `Q-learning` para que nuestro heroe 🙃 aprenda como obtener los trofeos 🏆!\n",
        "\n",
        "El martes veremos los detalles de como nace esta ecuacion basado en las *cadenas de Markov*, pero por mientras aqui les presentamos la formula y que significa cada termino de ella. La formula para aprender mediante `Q-learning` viene dada por la siguiente expresion:\n",
        "\n",
        "$$ Q^{nuevo}(s_t, a_t) = \n",
        "    \\underbrace{Q(s_t, a_t)}_{\\text{valor antiguo}} + \n",
        "    \\underbrace{\\alpha}_{\\text{la tasa de aprendizaje}} * \n",
        "        \\overbrace{\n",
        "            \\left(\\underbrace{r_t}_{\\text{recompensa}} + \n",
        "            \\underbrace{\\gamma}_{\\text{factor descuento}} * \n",
        "                \\underbrace{\\max_{acciones}(Q(s_{t+1},acciones))}_{\\text{valor optimo futuro}} -\n",
        "                \\underbrace{Q(s_t, a_t)}_{\\text{diferencia temporal}}\n",
        "            \\right)}\n",
        "        ^{\\text{lo que aprendimos}}\n",
        "$$\n",
        "\n",
        "Basicamente, para el estado actual, debemos ajustar el valor que nos dice que accion tomar basandonos en cual creemos que es el estado que nos da una mayor recompensa en el futuro. Es decir, desde el estado donde estamos, que accion nos lleva a un estado de mayor recompensa. El termino $Q(s_t, a_t)$ es importante porque representa la diferencia temporal del estado actual con el siguiente. No hay garantia que visitar 2 veces el mismo estado nos de la misma recompensa, por lo que hay que considerar que existe el tiempo en nuestra ecuacion.\n",
        "\n",
        "En el siguiente trozo de codigo implementamos esta ecuacion en un loop `for`. Para cada numero de `episodios`, avanzamos/ejecutamos hasta que el juego termina (porque ganamos o perdimos) o hasta que alcancemos el maximo de `pasos`. Es importante señalar que no todos los juegos *terminan*, asi que es importante que tengamos un limite hasta cuando queremos seguir. Tambien, si nos quedamos parados en el mismo lugar sin movernos, y no hay condiciones de tiempo, el juego durara para siempre. Hay que arreglar esos detalles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ITT2755S6LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def q_learning(start_state:State, episodes:int, steps:int, \n",
        "               table:Dict[State, np.ndarray], learning_rate:np.ndarray, \n",
        "               discount:float, nroItem) -> None:\n",
        "    \n",
        "    allReward = []\n",
        "    \n",
        "    for ep in range(episodes):\n",
        "        initCondition = StateCondition(nroItem=nroItem)\n",
        "        \n",
        "        # Creamos una copia para no modificar nuestro estado original\n",
        "        state = deepcopy(start_state)\n",
        "        \n",
        "        # Partimos con una recompensa 0\n",
        "        total_reward = 0\n",
        "        \n",
        "        # Cada episodio tiene una tasa de aprendizaje distinto\n",
        "        alpha = learning_rate[ep]\n",
        "\n",
        "        # Para cada paso, vamos a ir actualizando nuestra tabla\n",
        "        # para encontrar los movimientos que nos llevaran a ganar el juego\n",
        "        for _ in range(steps):\n",
        "            \n",
        "            # Tomamos una accion de nuestro banco de acciones\n",
        "            # dado nuestro estado\n",
        "            action = choose_action(state)\n",
        "            \n",
        "            # Llamamos un `acto`, para ver si lo hicimos bien\n",
        "            # necesitamos indicarle el estado donde estamos y la accion a\n",
        "            # realizar\n",
        "            # Esto nos da un nuevo estado, una recompensa y nos dice\n",
        "            # si se termino el juego o no.            \n",
        "            next_state, reward, done = act(state, action, initCondition)\n",
        "            \n",
        "            # Vamos guardando nuestras recompensas\n",
        "            total_reward += reward\n",
        "\n",
        "            # Actualizamos nuestros estados con la formula que vimos antes\n",
        "            q(state)[action] = q(state, action) + \\\n",
        "                alpha * (reward + gamma * np.max(q(next_state)) - q(state, action))\n",
        "            \n",
        "            # estamos listos para el siguiente paso\n",
        "            state = next_state\n",
        "            \n",
        "            # si el juego termino, dejamos los pasos y comenzamos con un\n",
        "            # nuevo episodio\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        print(f\"Episode {ep + 1}: total reward -> {total_reward}\")\n",
        "        allReward.append(total_reward)\n",
        "        \n",
        "    return allReward\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qshfYj-VqKJm",
        "colab_type": "text"
      },
      "source": [
        "## Listos para jugar!\n",
        "\n",
        "Ahora que ya tenemos nuestro estado, nuestro mapa, sabemos cuando darle recompensas a nuestro **agente**, y ademas implementamos la formula de arriba para aprender, solo nos queda ejecutar nuestro programa y ver como nuestro heroe 🙃 esquiva los zombies 🧟 para obtener el trofeo 🏆!\n",
        "\n",
        "- [x] Mapa\n",
        "- [x] Definicion de un estado\n",
        "- [x] Cuando dar recompensas y cuando castigar\n",
        "- [x] Aprender...\n",
        "\n",
        "Creemos nuestro mapa :-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca_6LCApMzbJ",
        "colab_type": "code",
        "outputId": "90dfb5eb-f828-4b95-b6d1-f2a2b0d4d251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Dibujamos nuestro mapa. Pueden dibujar lo que quieran, a ver si el\n",
        "# agente se la puede\n",
        "grid = [\n",
        "    [TROPHIE, EMPTY, EMPTY,EMPTY],\n",
        "    [ZOMBIE, ZOMBIE, ZOMBIE, EMPTY],\n",
        "    [BLOCK, BLOCK, BLOCK, EMPTY],\n",
        "    [EMPTY, EMPTY, TROPHIE, EMPTY]\n",
        "]\n",
        "\n",
        "\n",
        "# Y aqui definimos nuestro estado inicial. Pueden poner al heroe donde\n",
        "# quieran, pero cuiden que no parte sobre un zombie!\n",
        "initial_state = State(grid=grid, hero_pos=(1, 1))\n",
        "\n",
        "print(initial_state)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 ⚪ ⚪ ⚪\n",
            "🧟 🧟 🧟 ⚪\n",
            "🧱 🧱 🧱 ⚪\n",
            "🙃 ⚪ 🏆 ⚪\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "230K8AGatwRi",
        "colab_type": "code",
        "outputId": "95bf84d7-5b98-4fe4-97e3-f78a63818bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Escenario con puerta\n",
        "grid = [\n",
        "    [TROPHIE, EMPTY, EMPTY,EMPTY],\n",
        "    [ZOMBIE, ZOMBIE, ZOMBIE, EMPTY],\n",
        "    [BLOCK, BLOCK, BLOCK, EMPTY],\n",
        "    [KEY, EMPTY, BLOCK, DOOR],\n",
        "    [BLOCK, EMPTY, BLOCK, EMPTY],\n",
        "    [EMPTY, EMPTY, TROPHIE, EMPTY]\n",
        "]\n",
        "\n",
        "\n",
        "# Y aqui definimos nuestro estado inicial. Pueden poner al heroe donde\n",
        "# quieran, pero cuiden que no parte sobre un zombie!\n",
        "initial_state = State(grid=grid, hero_pos=(1, 1))\n",
        "\n",
        "print(initial_state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 ⚪ ⚪ ⚪\n",
            "🧟 🧟 🧟 ⚪\n",
            "🧱 🧱 🧱 ⚪\n",
            "🔑 ⚪ 🧱 🚪\n",
            "🧱 ⚪ 🧱 ⚪\n",
            "🙃 ⚪ 🏆 ⚪\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0uynVbZuBBN",
        "colab_type": "code",
        "outputId": "c41b63d1-7771-428b-caeb-316d5fb3070d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Escenario con puerta y espada\n",
        "grid = [\n",
        "    [TROPHIE, ZOMBIE, EMPTY,EMPTY],\n",
        "    [ZOMBIE, ZOMBIE, ZOMBIE, EMPTY],\n",
        "    [BLOCK, BLOCK, BLOCK, EMPTY],\n",
        "    [SWORD, DOOR, EMPTY, EMPTY],\n",
        "    [BLOCK, BLOCK, BLOCK, EMPTY],\n",
        "    [KEY, EMPTY, BLOCK, EMPTY],\n",
        "    [BLOCK, EMPTY, BLOCK, EMPTY],\n",
        "    [EMPTY, EMPTY, TROPHIE, EMPTY]\n",
        "]\n",
        "\n",
        "\n",
        "# Y aqui definimos nuestro estado inicial. Pueden poner al heroe donde\n",
        "# quieran, pero cuiden que no parte sobre un zombie!\n",
        "initial_state = State(grid=grid, hero_pos=(1, 1))\n",
        "\n",
        "print(initial_state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 🧟 ⚪ ⚪\n",
            "🧟 🧟 🧟 ⚪\n",
            "🧱 🧱 🧱 ⚪\n",
            "🗡️ 🚪 ⚪ ⚪\n",
            "🧱 🧱 🧱 ⚪\n",
            "🔑 ⚪ 🧱 ⚪\n",
            "🧱 ⚪ 🧱 ⚪\n",
            "🙃 ⚪ 🏆 ⚪\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsjOyKwLqrM2",
        "colab_type": "code",
        "outputId": "d8c6fe25-e6fb-43b4-ee30-6f23e2ebea3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# escenario con pared para probar el cruce de limites\n",
        "grid = [\n",
        "    [TROPHIE, BLOCK, EMPTY,EMPTY],\n",
        "    [ZOMBIE, BLOCK, ZOMBIE, EMPTY],\n",
        "    [BLOCK, BLOCK, BLOCK, EMPTY],\n",
        "    [SWORD, BLOCK, EMPTY, EMPTY],\n",
        "    [BLOCK, BLOCK, BLOCK, EMPTY],\n",
        "    [KEY, BLOCK, BLOCK, EMPTY],\n",
        "    [BLOCK, BLOCK, BLOCK, EMPTY],\n",
        "    [EMPTY, BLOCK, TROPHIE, EMPTY]\n",
        "]\n",
        "\n",
        "# Y aqui definimos nuestro estado inicial. Pueden poner al heroe donde\n",
        "# quieran, pero cuiden que no parte sobre un zombie!\n",
        "initial_state = State(grid=grid, hero_pos=(1, 1))\n",
        "\n",
        "print(initial_state)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 🧱 ⚪ ⚪\n",
            "🧟 🧱 🧟 ⚪\n",
            "🧱 🧱 🧱 ⚪\n",
            "🗡️ 🧱 ⚪ ⚪\n",
            "🧱 🧱 🧱 ⚪\n",
            "🔑 🧱 🧱 ⚪\n",
            "🧱 🧱 🧱 ⚪\n",
            "🙃 🧱 🏆 ⚪\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoFxnACXqsNE",
        "colab_type": "code",
        "outputId": "92103d33-b33c-4a47-81ee-e06e5617326d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Escenario entregado en la guia\n",
        "\n",
        "grid = [\n",
        "    [TROPHIE, EMPTY, EMPTY, EMPTY, ZOMBIE, TROPHIE, BLOCK, BLOCK],\n",
        "    [ZOMBIE, ZOMBIE, BLOCK, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
        "    [DOOR, EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
        "    [TROPHIE, BLOCK, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
        "    [ZOMBIE, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, BLOCK],\n",
        "    [EMPTY, EMPTY, BLOCK, EMPTY, BLOCK, BLOCK, EMPTY, BLOCK],\n",
        "    [EMPTY, ZOMBIE, BLOCK, KEY, BLOCK, BLOCK, EMPTY, BLOCK],\n",
        "    [EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, SWORD, BLOCK]\n",
        "]\n",
        "\n",
        "\n",
        "# Y aqui definimos nuestro estado inicial. Pueden poner al heroe donde\n",
        "# quieran, pero cuiden que no parte sobre un zombie!\n",
        "initial_state = State(grid=grid, hero_pos=(1, 1))\n",
        "\n",
        "print(initial_state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "🙃 ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePMUYCWDqs8o",
        "colab_type": "code",
        "outputId": "be2d9496-8aac-4b64-d1d8-674928842553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Escenario entregado en la guia con cambio de bloques por espacios para probar la\n",
        "# implementacion del traspaso de paredes\n",
        "\n",
        "grid = [\n",
        "    [TROPHIE, EMPTY, EMPTY, EMPTY, ZOMBIE, TROPHIE, EMPTY, EMPTY],\n",
        "    [ZOMBIE, ZOMBIE, BLOCK, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
        "    [DOOR, EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
        "    [TROPHIE, BLOCK, EMPTY, EMPTY, BLOCK, BLOCK, EMPTY, EMPTY],\n",
        "    [ZOMBIE, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, BLOCK],\n",
        "    [EMPTY, EMPTY, BLOCK, EMPTY, BLOCK, BLOCK, EMPTY, BLOCK],\n",
        "    [EMPTY, ZOMBIE, BLOCK, KEY, BLOCK, BLOCK, EMPTY, BLOCK],\n",
        "    [EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, SWORD, BLOCK]\n",
        "]\n",
        "\n",
        "\n",
        "# Y aqui definimos nuestro estado inicial. Pueden poner al heroe donde\n",
        "# quieran, pero cuiden que no parte sobre un zombie!\n",
        "initial_state = State(grid=grid, hero_pos=(1, 1))\n",
        "\n",
        "print(initial_state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 ⚪ ⚪\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 ⚪ ⚪\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "🙃 ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNTHJ-HW0L6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vamos a dejar esto aqui para que lo completen en la pregunta 2!\n",
        "\n",
        "# RELLENE AQUI PARA RESPONDER A LA PREGUNTA 2\n",
        "class StateCondition:\n",
        "    def __init__(self,nroItem:int): # quizas recibe algo?\n",
        "      self.nroItem = nroItem\n",
        "      self.door = False\n",
        "      self.sword = False\n",
        "      pass\n",
        "        \n",
        "    # esto nos permite llamar al objeto como si fuera una funcion\n",
        "    def __call__(self, interaction:GridElement) -> Tuple[int, bool]:\n",
        "        \n",
        "        if interaction == TROPHIE:\n",
        "            self.nroItem = self.nroItem-1\n",
        "            return 1000,self.nroItem == 0\n",
        "        \n",
        "        if interaction == ZOMBIE:\n",
        "            \n",
        "            if(self.sword):\n",
        "              return 10,False\n",
        "            else:\n",
        "              return -1000,True\n",
        "        \n",
        "        if interaction == EMPTY:\n",
        "            \n",
        "            return -1,False\n",
        "          \n",
        "        if interaction == HERO:\n",
        "            \n",
        "            return -10,False\n",
        "          \n",
        "        if interaction == BLOCK:\n",
        "            \n",
        "            return -10,False\n",
        "        \n",
        "        if interaction == KEY:\n",
        "            \n",
        "            self.door = True\n",
        "            return 100,False\n",
        "          \n",
        "        if interaction == DOOR:\n",
        "            \n",
        "            learn = -10\n",
        "            if(self.door):\n",
        "              learn = 100000\n",
        "            return learn,False\n",
        "          \n",
        "        if interaction == SWORD:\n",
        "            \n",
        "            self.sword = True\n",
        "            return 100,False\n",
        "        \n",
        "        return -1,False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTbdoDpls8xS",
        "colab_type": "text"
      },
      "source": [
        "## Ejecutando las guias de aprendizaje\n",
        "\n",
        "A continuacion vamos a llamar a la funcion que hara que nuestra `q_table` se llene de informacion util y representativa del juego que queremos ganar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RVj4G6bwDWx",
        "colab_type": "code",
        "outputId": "86824ef9-57d6-40df-aa74-6cdaa2ce2b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "source": [
        "# por si acaso, antes de entrenar reiniciamos nuestra tabla para no tener\n",
        "# informacion demas\n",
        "reset_table()\n",
        "\n",
        "nroItem = 3\n",
        "# Le pasamos los argumentos a nuestra funcion de aprendizaje y estamos\n",
        "# listos para ver los resultados de nuestro agente\n",
        "\n",
        "qAcumulado = q_learning(start_state=initial_state, \n",
        "           episodes=N_EPISODES, \n",
        "           steps=MAX_EPISODE_STEPS, \n",
        "           table=q_table, \n",
        "           learning_rate=alphas, \n",
        "           discount=gamma,\n",
        "           nroItem=nroItem)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 1: total reward -> -1016\n",
            "Episode 2: total reward -> -1001\n",
            "Episode 3: total reward -> 101058\n",
            "Episode 4: total reward -> 100979\n",
            "Episode 5: total reward -> 101060\n",
            "Episode 6: total reward -> 102942\n",
            "Episode 7: total reward -> -1005\n",
            "Episode 8: total reward -> 101050\n",
            "Episode 9: total reward -> -34\n",
            "Episode 10: total reward -> 100756\n",
            "Episode 11: total reward -> -1003\n",
            "Episode 12: total reward -> -1020\n",
            "Episode 13: total reward -> 103103\n",
            "Episode 14: total reward -> 103122\n",
            "Episode 15: total reward -> -1007\n",
            "Episode 16: total reward -> 102881\n",
            "Episode 17: total reward -> 101042\n",
            "Episode 18: total reward -> -1024\n",
            "Episode 19: total reward -> -1024\n",
            "Episode 20: total reward -> 42\n",
            "Episode 21: total reward -> 103102\n",
            "Episode 22: total reward -> 40\n",
            "Episode 23: total reward -> -1002\n",
            "Episode 24: total reward -> -66\n",
            "Episode 25: total reward -> 103116\n",
            "Episode 26: total reward -> -24\n",
            "Episode 27: total reward -> -36\n",
            "Episode 28: total reward -> -1004\n",
            "Episode 29: total reward -> -1013\n",
            "Episode 30: total reward -> -45\n",
            "Episode 31: total reward -> 52\n",
            "Episode 32: total reward -> 103120\n",
            "Episode 33: total reward -> 103142\n",
            "Episode 34: total reward -> -1044\n",
            "Episode 35: total reward -> 103126\n",
            "Episode 36: total reward -> 103144\n",
            "Episode 37: total reward -> 103154\n",
            "Episode 38: total reward -> -1013\n",
            "Episode 39: total reward -> -17\n",
            "Episode 40: total reward -> 3069\n",
            "Episode 41: total reward -> -1004\n",
            "Episode 42: total reward -> -1026\n",
            "Episode 43: total reward -> 103170\n",
            "Episode 44: total reward -> 103122\n",
            "Episode 45: total reward -> -1010\n",
            "Episode 46: total reward -> 102971\n",
            "Episode 47: total reward -> 100052\n",
            "Episode 48: total reward -> 103134\n",
            "Episode 49: total reward -> 103176\n",
            "Episode 50: total reward -> 103088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_H0UslQxm3b",
        "colab_type": "text"
      },
      "source": [
        "## Viendo los resultados\n",
        "\n",
        "Bueno, la funcion de antes me mostro algunos numeros pero se si en verdad el heroe 🙃 habra logrado su comentido!\n",
        "Ejecuta el codigo de abajo para que se muestre lo aprendido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgAnCFsOxl3V",
        "colab_type": "code",
        "outputId": "4580096d-40a6-4a3f-a14e-f768be06cd4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Solo en caso de que nuestro heroe se quede pegado, ponemos un maximo\n",
        "# de escenarios a mostrar, aqui tenemos maximo 100\n",
        "show_max = 100\n",
        "\n",
        "stateCondition = StateCondition(nroItem=nroItem)\n",
        "\n",
        "# Partimos con el estado inicial, preguntemos a donde podemos movernos\n",
        "possible_actions = q(initial_state)\n",
        "print(initial_state)\n",
        "print(f\"up={possible_actions[UP]}, \"\n",
        "      f\"down={possible_actions[DOWN]}, \"\n",
        "      f\"left={possible_actions[LEFT]}, \"\n",
        "      f\"right={possible_actions[RIGHT]}\")\n",
        "\n",
        "# Seleccionamos la accion con el mejor puntaje, y le pedimos al\n",
        "# heroe que se mueva en esa direccion. Esto nos da un nuevo estado\n",
        "s, _, done = act(initial_state, np.argmax(possible_actions),stateCondition)\n",
        "\n",
        "# Mientras no haya terminado el juego, o nos hayamos pasado del maximo de\n",
        "# acciones a mostar definido mas arriba, seguimos moviendonos con la \n",
        "# accion mas favorable para ese estado\n",
        "while not done and show_max:\n",
        "    # Mostramos el estado actual\n",
        "    print(s)\n",
        "    # vemos nuestras acciones\n",
        "    possible_actions = q(s)\n",
        "    # Que accion deberiamos tomar?\n",
        "    print(f\"up={possible_actions[UP]}, \"\n",
        "      f\"down={possible_actions[DOWN]}, \"\n",
        "      f\"left={possible_actions[LEFT]}, \"\n",
        "      f\"right={possible_actions[RIGHT]}\")\n",
        "    # Elejimos la mejor accion y continuamos\n",
        "    s, _, done = act(s, np.argmax(possible_actions),stateCondition)\n",
        "    \n",
        "    show_max -= 1\n",
        "print(s)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "🙃 ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=266.33851825022657, down=136.3261014444795, left=155.24396344730508, right=86.76398351780581\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "🙃 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=300.74227752902055, down=183.8918601900315, left=192.07795797610015, right=-999.9999909525967\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "🙃 ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-1000.0, down=196.54882992180274, left=204.1086797725233, right=338.05893251189195\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ 🙃 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=378.65107845543673, down=-999.9854817993164, left=187.03837114353274, right=222.90449246371554\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 🙃 ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=206.11619689454855, down=234.03938446014925, left=-999.999515856179, right=422.83141337245746\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ 🙃 ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=471.21999382632447, down=323.2547494898466, left=288.65417560169067, right=318.5457124719759\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 🙃 ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=17.547477389298315, down=325.2926168751887, left=346.6706280138529, right=524.7051465191038\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ 🙃 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=584.0205968927193, down=277.9523215549335, left=386.46941808711426, right=123.08056414541969\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ 🙃 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=649.9370163271514, down=113.09547133155218, left=104.51707537385123, right=473.1088672673435\n",
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 🙃 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=723.2179747481733, down=383.6666218774268, left=495.7228849841798, right=60.4047119320303\n",
            "🏆 ⚪ ⚪ 🙃 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=630.6640171986231, down=323.4010456567001, left=804.7144595386942, right=-996.7328075064589\n",
            "🏆 ⚪ 🙃 ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=87.46284150822567, down=9.1588655920261, left=895.4167969106405, right=501.27159437856363\n",
            "🏆 🙃 ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-7.421052631578949, down=-880.8526315789474, left=996.5988166306589, right=0.0\n",
            "🙃 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-13.366547765766581, down=-983.6611154544045, left=-6.320854631422358, right=-2.0963303296295055\n",
            "⚪ 🙃 ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-8.31568055847918, down=-985.4172954580883, left=-4.583125804444971, right=2.71497116273313\n",
            "⚪ ⚪ 🙃 ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-11.410386302099731, down=-10.22707339299605, left=-4.510826299989501, right=13.322170330345983\n",
            "⚪ ⚪ ⚪ 🙃 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-7.249515789473685, down=27.88129091203695, left=-2.5832178236140653, right=-554.3578947368421\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 🙃 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-3.485280305008215, down=45.60794300240361, left=-9.6, right=-12.129542775152768\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ 🙃 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=6.579496883151788, down=59.632051294921304, left=-2.4174980050677757, right=-9.004951040000002\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ 🙃 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-1.29736964099584, down=75.11737791110642, left=-1.46624, right=-10.821952\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ 🙃 ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=3.149252665663999, down=86.65725743550368, left=1.7422363648149943, right=14.129099667939833\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 🙃 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=2.753292005226946, down=97.42140150497876, left=0.0, right=58.96617331594214\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🙃 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-3.001714211271789, down=-10.131261856080606, left=-7.9291369380841115, right=-11.521809872630659\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 🙃 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-2.4007682492346194, down=-3.2326209616673927, left=-11.418352046238786, right=-9.605843654085376\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ 🙃 ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-2.045003156992, down=-2.486191616881869, left=-1.7987184880428115, right=-1.825779375817606\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ 🙃 ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=2.7380151640020367, down=-4.6, left=-1.3506697676800001, right=-1.32916\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 🙃 ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=224.50521907359484, down=-1.328896, left=-9.399999999999999, right=-1.3518908799999998\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ 🙃 ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-9.856, down=2.0128833609939436, left=3370.5199147581434, right=-1.6267724041600002\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 🙃 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-940.0, down=0.0, left=30041.96007616, right=0.0\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-140.0, down=186.39059929599995, left=0.0, right=0.0\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-79.99999999999996, down=-0.08842420664320004, left=-0.7999999999999996, right=-0.09717496000000003\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n",
            "up=-0.20512972057599999, down=-59.99999999999994, left=-0.5999999999999994, right=-0.5999999999999994\n",
            "⚪ ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🙃 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "⚪ 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnzUuNcQ1B2u",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Preguntas\n",
        "\n",
        "Esperamos haya sido entretenido ver como nuestro heroe 🙃 lograba esquivar los zombies 🧟 para llegar al trofeo 🏆. Ahora les toca a ustedes mejorar lo que les mostramos y responder las siguientes preguntas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3W2WKDTiDVu",
        "colab_type": "code",
        "outputId": "70b4026d-9cb4-424c-cf29-ca8b90dc7864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(initial_state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "🙃 ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTm4i4km1gzh",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 1: haciendo una curva de aprendizaje\n",
        "En esta pregunta les pedimos que dibujen un grafico, usando `matplotlib`, donde se pueda ver como fue cambiando la **recompensa** total por cada episodio que iba pasando. Lo que queremos ver es que la recompensa debe partir baja, pero a medida que pasan los episodios, esta deberia subir hasta que muestre que siempre gana el juego. O quizas suba y baje todo el rato... Por que ocurre esto?\n",
        "\n",
        "> Hagan un grafico de recompensa por episodio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9dDMBSkHKpu",
        "colab_type": "code",
        "outputId": "cc712f53-26f0-41a9-88f2-7747d12448e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "# ESCRIBA AQUI SU RESPUESTA A LA PREGUNTA 1\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print(qAcumulado)\n",
        "\n",
        "plt.plot(qAcumulado)\n",
        "plt.title('bla bla')\n",
        "plt.xlabel('Episodios')\n",
        "plt.ylabel('Premio')\n",
        "plt.show()\n",
        "\n",
        "print(\"La grafica muestra el proceso de aprendizaje, donde la maquina busca diferentes alternativas \\\n",
        "al problema, la idea de este es equivocarse y acertar en busqueda del camino correcto, \\\n",
        "por tal razon el reward va cambiando de positivoa negativo durante el transcurso de los episodios\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "La grafica muestra el proceso de aprendizaje, donde la maquina busca diferentes alternativas al problema, la idea de este es equivocarse y acertar en busqueda del camino correcto, por tal razon el reward va cambiando de positivoa negativo durante el transcurso de los episodios\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmNDV-oj3B4y",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 2: solo un trofeo?\n",
        "\n",
        "El juego esta muy simple, obvio que puede lograrlo... Que pasa si agregamos otro trofeo 🏆, y la condicion para ganar es que debe recoger **ambos** trofeos 🏆🏆 para terminar el juego?\n",
        "\n",
        "Completa la clase `StateCondition` para implementar este funcionamiento. Ademas, debes agregar algunos argumentos a algunas funciones para que estas sepan cual es la condicion de terminacion del juego, que funciones se encargan de eso?\n",
        "\n",
        "Como ayudas considera que necesitas saber en todo momento si ya se tomo un trofeo o no, asi que debes guardar esa informacion. Ademas, la clase `StateCondition` en su metodo `__call__` recibe un elemento del juego con el que interactua el heroe, y retorna una recompensa y un valor que indica si el juego termino o no.\n",
        "Finalmente, recuerda que hay 2 funciones que se involucran con la condicion de terminacion y con las recompensas.\n",
        "\n",
        "> Implementen lo necesario para que se necesiten 2 trofeos para terminar el juego. Rellenen el codigo en las secciones anteriores, y modifiquen lo necesario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuYuABJC-Pfk",
        "colab_type": "text"
      },
      "source": [
        "##Algunos antecedentes de la pregunta 2:\n",
        "*El heroe, la instanciación del StateCondition(nroItem=nroItem) se realiza dentro de la función q_learning, pasado el for.\n",
        "Esto permite que cada vez que se inicie una partida, los trofeos comienzan de cero, por lo que nuestro heroe gana solo cuando llega a los 2 trofeos, es decir, si toma uno, se devuelve en el camino para conseguir el otro, si mientras va en búsqueda del otro trofeo encuentra un zombie, el héroe muere y empezará de nuevo. Esto ayuda a que, en los siguientes pasos de la tarea, el personaje pueda aprender, dado que no lo confunde con la captura de llave ni matanzas a los zombies.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqFOk834yCd",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 3: agreguemos mas cosas!!\n",
        "\n",
        "Nuestro juego ya tiene zombies 🧟, trofeos 🏆 y a nuestro heroe 🙃. Ademas, agregamos esta condicion extra para necesitar mas de 1 trofeo para terminar el juego. Es momento de hacerlo mas entretenido.\n",
        "\n",
        "Ahora deberan agregar los siguientes elementos al juego:\n",
        "\n",
        "1.   Bloques 🚫: el heroe 🙃 no puede pasar por aqui, es lo mismo a chocar con una muralla.\n",
        "2.   Puerta 🚪: un tipo de bloqueo, pero que puede ser desbloqueado consiguiendo la llave que abre la puerta. El heroe 🙃 no puede pasar por aqui hasta que consigua la llave 🔑.\n",
        "3.   Llave 🔑: un objeto que esta en alguna parte del mapa. Una vez conseguimos este objeto podemos abrir una de las puertas del juego.\n",
        "\n",
        "Ayudas e indicaciones:\n",
        "\n",
        "*   Los bloques 🚫 son lo mismo que una muralla, solo se ven distintos y, claro, no estan en los extremos del mapa. No hay forma de destruirlos y no impactan al jugador, solo estan ahi para estorbar.\n",
        "*   Puedes considerar que una vez abierta la puerta 🚪, esta desaparece. No te preocupes de esto ya que no es la idea de la actividad. Solo debes implementar la condicion que la puerta no puede atravesarse hasta conseguir la llave 🔑.\n",
        "*   Para la llave 🔑 hay 2 opciones. Puedes considerar que solo se puede usar una vez y luego para abrir una segunda puerta hay que buscar una nueva, o que una sirve para todas. Esto no es lo importante, asi que puedes implementar lo que consideres mas entretenido :-)\n",
        "*   Estos nuevos elementos se llaman `BLOCK` 🚫, `DOOR` 🚪 y `KEY` 🔑, y fueron definidos al principio de este documento. Debes modificar las funciones que interactuan con estos elementos, y tambien la clase que ve las condiciones de juego. En la clase `StateCondition` puedes agregar si el heroe tiene ya la llave o no.\n",
        "\n",
        "> Implementen los Bloques 🚫, Puertas 🚪 y Llaves 🔑 en el juego. Una puerta solo puede abrirse una vez conseguida una llave.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20QzNGh_-JTK",
        "colab_type": "text"
      },
      "source": [
        "Hasta aqui llega el primer dia! Algunas cosas para que se vayan pensado:\n",
        "\n",
        "*   Funciona con tantos elementos el juego tal y como lo definimos?\n",
        "*   Esta aprendiendo nuestro heroe todas estas mecanicas complejas del juego? Como podriamos ayudar?\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "Desde aqui continuamos: dia 2!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMDtUWJn8Zx8",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 4: hora de enfrentarse a los zombies 🧟\n",
        "\n",
        "Hasta ahora solo hemos evitado a los zombies 🧟, pero ya no mas! Es hora de hacerles frente, asi que agregaremos una espada 🗡️ al juego. Los zombies siguen siendo peligrosos al heroe 🙃 si esta desarmado, pero una vez el heroe 🙃 consigue la espada 🗡️ puede enfrentarlos. Cuando el heroe consigue la espada 🗡️, si toca a un zombie este desaparece del mapa y da una recompensa al heroe, algo asi como puntos extra.\n",
        "\n",
        "Ayudas:\n",
        "\n",
        "*   Primero debes agregar la espada 🗡️ al juego, y luego sumarla a las condiciones de estado de `StateCondition`. Es muy similar a la llave 🔑 y puerta 🚪, solo que ahora el zombie ya existia.\n",
        "\n",
        "> Implementen la espada 🗡️ en el juego. Cuando el heroe 🙃 tiene la espada 🗡️, puede vencer facilmente a los zombies 🧟."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viX6Vaoa-KnJ",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 5: no funciona...\n",
        "\n",
        "Nuestro heroe no esta siendo capaz de aprender a jugar el nuevo juego despue de agregar tantas cosas. Hemos hecho el juego demasiado dificil?\n",
        "\n",
        "> Indiquen que cosas hay que cambiar en las configuraciones del aprendizaje para que ahora si podamos ganar. Intenten con el mapa que dejamos mas abajo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLGLEy0q_E98",
        "colab_type": "text"
      },
      "source": [
        "##Respuesta a **pregunta 5**.\n",
        "\n",
        "\n",
        "*Respecto al desarrollo de la pregunta 5, esta se encuentra en la parte principal. Al agregar más objetos al héroe se le complejiza la tarea, por lo que no termina el juego, ya que prefería quedarse quieto , porque no tiene holgura de movimientos(pasos), quedando sin alternativa de desplazarse. Para solucionar esta dificultad, se aumentaron los pasos y los episodios, que permitió finalmente que el héroe pudiera capturar ambos trofeos.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQynZn5bCQBz",
        "colab_type": "code",
        "outputId": "2459bb6e-b9dd-4e6b-d177-3a40a9015db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "grid = [\n",
        "    [TROPHIE, EMPTY, EMPTY, EMPTY, ZOMBIE, TROPHIE, BLOCK, BLOCK],\n",
        "    [ZOMBIE, ZOMBIE, BLOCK, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
        "    [DOOR, EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
        "    [TROPHIE, BLOCK, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
        "    [ZOMBIE, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, BLOCK],\n",
        "    [EMPTY, EMPTY, BLOCK, EMPTY, BLOCK, BLOCK, EMPTY, BLOCK],\n",
        "    [EMPTY, ZOMBIE, BLOCK, KEY, BLOCK, BLOCK, EMPTY, BLOCK],\n",
        "    [EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, SWORD, BLOCK]\n",
        "]\n",
        "print(Grid(grid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🏆 ⚪ ⚪ ⚪ 🧟 🏆 🧱 🧱\n",
            "🧟 🧟 🧱 ⚪ 🧱 🧱 🧱 🧱\n",
            "🚪 ⚪ ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🏆 🧱 ⚪ ⚪ 🧱 🧱 🧱 🧱\n",
            "🧟 ⚪ ⚪ ⚪ ⚪ ⚪ ⚪ 🧱\n",
            "⚪ ⚪ 🧱 ⚪ 🧱 🧱 ⚪ 🧱\n",
            "⚪ 🧟 🧱 🔑 🧱 🧱 ⚪ 🧱\n",
            "⚪ ⚪ ⚪ 🧱 🧱 🧱 🗡️ 🧱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUOKT2fh_KR6",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 6: no funcionaba... por que?\n",
        "\n",
        "> Por que hubo que cambiar esos parametros en la pregunta 5? Den un comentario respecto a lo que hacen esos parametros y por que cambiarlos arreglo todo nuestro problema\n",
        "\n",
        "Ayudas:\n",
        "\n",
        "*   Cuando juegas un juego, entiendes todo a la primera, o hay que intentarlo varias veces para acordarse?\n",
        "*   Al jugar algo, o aprender una nueva habilidad, hay que practicarlo miles de veces 1 segundo, o unas 100 veces pero mas tiempo?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucRN_Ugw_hCS",
        "colab_type": "text"
      },
      "source": [
        "##Respuesta a **pregunta 6**.\n",
        "\n",
        "*Al cambiar estos parámetros, permite que el héroe tenga más oportunidad de aprender, dado que le da más opciones para cometer errores (\"holgura\"), y si ganas el juego o llegas a la respuesta satisfactoria, debes repetir una y otra vez para obtener el conocimiento necesario que permita enfrentarse a la misma situación de manera victoriosa. Por esta razón es importante dar la holgura suficiente en el límite de movimientos(pasos)  y repetir(episodios) o iterar de manera adecuada para enfrentar cualquier problema que se presenta.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYwrztUy_wyx",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 7: Recompensa negativa al movernos\n",
        "\n",
        "Durante todo este tiempo, en la funcion `act`, cada vez que nuestro heroe se movia a un lugar vacio, es decir, se movia respetando todas las reglas, le dabamos una recompensa de `-1`. Especificamente asi:\n",
        "```python\n",
        "...\n",
        "elif grid_item == EMPTY:\n",
        "    reward = -1\n",
        "    is_done = False\n",
        "...\n",
        "```\n",
        "Esto quiere decir que hemos estado castigando al heroe cada vez que se mueve... Por que? Podriamos borrar esa condicion y dar una recompensa positiva? Una recompensa de 0? Comenten sobre que opinan de esto y para que creen que sirve.\n",
        "\n",
        "> Por que la recompensa de moverse es negativa? Que pasaria si la cambiamos a una recompensa positiva, o 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEc8J5D1AzUJ",
        "colab_type": "text"
      },
      "source": [
        "Respuesta a **pregunta 7**.\n",
        "\n",
        "*Una buena alternativa sería dar premios positivos por moverse a los espacios vacíos, con esto se asegura que recorra la mayor área del mapa hasta llegar al trofeo, generando un camino más largo pero efectivo, en puntos finales. Ahora dependerá del monto del premio, si el héroe tendrá incentivo a quedarse quieto o moverse. Recompenza igual igual a cero se puede dar el caso que prefiera o sea más beneficioso estando en el mismo lugar o como en el caso empirico,concentrarse más en destruir los zoombies y guardar la recompenza para el final, cuando pusimo cero de premio en los espacios vacíos, el personaje buscaba matar a todos los zombies y recorrer la mayor cantidad de espacios y esperando al final para alcanzar los trofeos.*\n",
        "\n",
        "*El algoritmo trata de aumentar la gancia, el recorrer más y destruir más enemigos corre en contra de ésta cuando la recompenza es negativa, buscar los elementos que me generan mayor gasto, pero siempre dependiendo de aquellos elementos que me pongan a prueba, ejemplo, si recojo la espada es debido a que tuvo importancia en algún momento a diferencia de o recoger la llave, dado que nunca tuvo la necesidad de probarla, a pesar de haber recibido 1 millon en recompenza.\n",
        "Por otro lado, si subíamos el monto del premio, a 100 por no moverse, este prefería quedarse en su lugar dado que resultaba más lucrativo que aventurarse a moverse entre las grillas, arriesgando quizás toparse con un zombie.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlTKr1F4A0tj",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta Extra: murallas fijas\n",
        "\n",
        "Siempre hemos considerado que las murallas del mapa son fijas y no podemos cruzarlas, pero, que ocurre si pudieramos atravesarlas y aparecer al otro lado? Implemente este cambio y cuente sus resultados.\n",
        "\n",
        "> Implemente el poder atravesar uno de los limites del mapa y aparecer en el otro lado.\n",
        "\n",
        "Aclaracion: esta es una pregunta opcional y no es requerido responderla para la nota (pero puede haber algo extra ;-).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoyiCywHBQUJ",
        "colab_type": "text"
      },
      "source": [
        "##Respuesta a **pregunta extra**.\n",
        "\n",
        "*El Heroe aprende a cruzar las paredes y iba directo a los objetivos.*\n",
        "\n",
        "*Como la idea es maximizar beneficios, a través de obtener los trofeos, al dar un premio negativo a los espacios vacíos, permite que el héroe se enfoque en llegar a los trofeos, de manera directa, y si atravezar le ayudaba a llegar al objetivo de manera más rapida, lo hacia, como metodologia para terminar el juego en menos pasos, y no se confunda en el camino, evitando que escoja un camino más largo. a diferencia, si el objetivo fuera recorrer los espacios y que sea un explorador, en este caso lo premiamos en cada paso.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69rSAU-uAyG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# La clase `State` representara un estado del heroe\n",
        "class State:\n",
        "    # En nuestro constructor solo asignamos nuestras variables\n",
        "    def __init__(self, grid:Union[Grid, List[List[GridElement]]]=None, \n",
        "                 hero_pos:Tuple[int,int]=(1,1)) -> None:\n",
        "        \n",
        "        self.grid = Grid(grid=grid)\n",
        "        self.x_lim, self.y_lim = self.grid.shape\n",
        "        self.hero_x, self.hero_y = hero_pos\n",
        "        \n",
        "    # Misma forma `fancy` de acceder a la posicion del heroe\n",
        "    @property\n",
        "    def hero_pos(self) -> Tuple[int, int]:\n",
        "        return (self.hero_x, self.hero_y)\n",
        "    \n",
        "    # Para dibujar nuestro mapa con el heroe en la posicion actual\n",
        "    def __str__(self) -> str:\n",
        "        grid = deepcopy(self.grid)\n",
        "        grid[self.hero_x, self.hero_y] = HERO\n",
        "        return grid.__str__()\n",
        "    \n",
        "    # Un estado es igual a otro si las grillas y posiciones de los heroes son\n",
        "    # las mismas\n",
        "    def __eq__(self, other:'State') -> bool:\n",
        "        return isinstance(other, State) and self.hero_pos == other.hero_pos and \\\n",
        "            self.grid == other.grid\n",
        "    \n",
        "    # Igual que antes, por completitud debemos implementar cuando 2\n",
        "    # estados tienen el mismo hash\n",
        "    def __hash__(self) -> int:\n",
        "        return hash(str(self.grid) + str(self.hero_pos))\n",
        "    \n",
        "    # Este metodo nos ayuda a obtener que elemento se encuentra en una posicion\n",
        "    # determinada. Necesitamos el estado pasado para comparar ya que no tenemos\n",
        "    # historia, pero es una forma simple de implementar el mapa sin\n",
        "    # mucho codigo. Recuerda que estamos en una cadena de Markov, aqui no tenemos\n",
        "    # los estados pasados! Estos no afectan la decision que tomaremos ahora.\n",
        "    def get_element(self, position:Tuple[int,int], state:'State') -> GridElement:\n",
        "        assert type(position) == tuple\n",
        "        assert len(position) == 2\n",
        "        x, y = position\n",
        "        assert 0 < x <= self.x_lim\n",
        "        assert 0 < y <= self.y_lim\n",
        "        \n",
        "        # Por limitaciones de la implementacion, debemos saber si el heroe se\n",
        "        # movio a la posicion en la que esta, o estaba ahi desde antes\n",
        "        # otra implementacion podria solucionar este problema de mejor manera\n",
        "        # pero es mas compleja de entender\n",
        "        if position == state.hero_pos:\n",
        "            return HERO\n",
        "        return self.grid[x,y]\n",
        "        \n",
        "    # De nuestras acciones tenemos que elegir una y actuar acorde a ella.\n",
        "    # por ejemplo, si le pedimos al estado que suba, entonces tenemos que \n",
        "    # enviar la accion `UP`.\n",
        "    # Cuando llamamos a este metodo, creamos un nuevo estado con la\n",
        "    # accion aplicada\n",
        "    def action_dispatch(self, action:Action) -> 'State':\n",
        "        if action == UP:\n",
        "            return self.moveUp()\n",
        "        elif action == DOWN:\n",
        "            return self.moveDown()\n",
        "        elif action == LEFT:\n",
        "            return self.moveLeft()\n",
        "        elif action == RIGHT:\n",
        "            return self.moveRight()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown action {action}\")\n",
        "        \n",
        "    # Este metodo solo copia el estado actual y crea uno nuevo para aplicar\n",
        "    # los cambios pedidos por la accion ingresada\n",
        "    def register(self) -> 'State':\n",
        "        past_state = copy(self)\n",
        "        return State(grid=past_state.grid, hero_pos=past_state.hero_pos)\n",
        "    \n",
        "    # Los siguientes metodos mueven nuestro personaje en las direcciones\n",
        "    # que definimos antes, arriba, abajo, derecha e izquierda\n",
        "        \n",
        "    def moveUp(self) -> 'State':\n",
        "        new_state = self.register()\n",
        "        #posicio =(new_state.hero_x, new_state.hero_y + 1 if new_state.hero_y < new_state.y_lim else new_state.hero_y)\n",
        "        #print(posicio)\n",
        "        #print(self.get_element(posicio,self))\n",
        "        new_state.hero_y = new_state.hero_y + 1 if new_state.hero_y < new_state.y_lim else 1\n",
        "        \n",
        "        return new_state\n",
        "        \n",
        "    def moveDown(self) -> 'State':\n",
        "        new_state = self.register()\n",
        "        #posicio =(new_state.hero_x, new_state.hero_y - 1 if new_state.hero_y > 1 else new_state.hero_y)\n",
        "        #print(posicio)\n",
        "        #print(self.get_element(posicio,self))\n",
        "        new_state.hero_y = new_state.hero_y - 1 if new_state.hero_y > 1 else new_state.y_lim\n",
        "        \n",
        "        return new_state\n",
        "        \n",
        "    def moveRight(self) -> 'State':\n",
        "        new_state = self.register()\n",
        "        #posicio =(new_state.hero_x + 1 if new_state.hero_x < new_state.x_lim else new_state.hero_x, new_state.hero_y)\n",
        "        #print(posicio)\n",
        "        #print(self.get_element(posicio,self))\n",
        "        new_state.hero_x = new_state.hero_x + 1 if new_state.hero_x < new_state.x_lim else 1\n",
        "        \n",
        "        return new_state\n",
        "        \n",
        "    def moveLeft(self) -> 'State':\n",
        "        new_state = self.register()\n",
        "        #posicio =(new_state.hero_x - 1 if new_state.hero_x > 1 else new_state.hero_x, new_state.hero_y)\n",
        "        #print(posicio)\n",
        "        #print(self.get_element(posicio,self))\n",
        "        new_state.hero_x = new_state.hero_x - 1 if new_state.hero_x > 1 else new_state.x_lim\n",
        "        \n",
        "        return new_state"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}